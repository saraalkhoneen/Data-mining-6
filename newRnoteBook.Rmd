```{r}
options(repos = c(CRAN = "https://cloud.r-project.org"))
```

### Project Motivation:

Credit risk assessment has become an increasingly critical part of the loan application process. As the demand for loan products increases, lenders need to be able to quickly and accurately determine the likelihood of a loan defaulting. In the past, credit risk assessment was a labor-intensive process that required lenders to manually assess each application. This process could be very time-consuming, as lenders had to analyze a variety of data points and review multiple documents. Manual credit risk assessment was also susceptible to human error, which could lead to inaccuracies in the assessment. Traditional methods of assessing credit risk can be inaccurate and may not take into account the full range of factors that can affect a borrower's ability to repay a loan. Moreover, they can be prone to human bias, which can lead to unfair lending practices. In recent years, advances in technology have enabled lenders to use more accurate and efficient methods of assessing credit risk.

### Data Mining Task:

Four methods we will explore are decision trees to classify and K-means, k-medoids and elbow methods to create clusters of the dataset. These methods are more accurate and efficient than traditional methods. They are able to take into account more factors in assessing credit risk, and are less prone to human bias. Additionally, they are able to process large amounts of data quickly, making them ideal for large-scale lending operations. By applying these algorithms to the credit risk dataset, we aim to develop a robust and accurate model. This can help financial institutions make more informed lending decisions.

Our primary goal is to classify customers as either good or bad credit risks. We aim to assist financial institutions in making informed decisions regarding loan approvals based on an analysis of customers' payment behavior and other relevant attributes, such as age, income, loan amount, and employment status. By utilizing this dataset, we will build predictive models to assess customers' creditworthiness and use these models as tools for risk assessment and decision-making.

We also intend to construct clusters for the dataset to understand our dataset better by excluding our class label (the class attribute) then applying the clustring algorithms on the dataset to recognize any patterns or trends between customers which will help to group them into subsets or (clusters) which make it easier to make decisions regarding one group. Four methods we will explore are decision trees to classify and K-means, k-medoids and elbow methods to create clusters of the dataset. These methods are more accurate and efficient than traditional methods. They are able to take into account more factors in assessing credit risk, and are less prone to human bias. Additionally, they are able to process large amounts of data quickly, making them ideal for large-scale lending operations. By applying these algorithms to the credit risk dataset, we aim to develop a robust and accurate model. This can help financial institutions make more informed lending decisions.

### source

The dataset was sourced from the kaggle website in this URL: <https://www.kaggle.com/datasets/ppb00x/credit-risk-customers>

## General information

Origonaly our dataset consists of 21 attributes, but we only worked with 21 attributes that are going to help with our study of the credit risks of applicants. 1- Number of Attributes: 21 2- Number of Objects: 1001 3- class name and lable: The "class" attribute which describes whether the customer has a good or bad credit risks.

## Attribute description

| Attribute name      | Data type | Description                                                                                                                                                                                                                                                                                   |
|---------|---------|------------------------------------------------------|
| checking_status     | Nominal   | status of existing checking acount of loan applicant                                                                                                                                                                                                                                          |
| duration            | Numeric   | duration of loan in months                                                                                                                                                                                                                                                                    |
| credit_history      | Nominal   | credit history of loan applicant                                                                                                                                                                                                                                                              |
| savings_status      | Nominal   | status of savings accounts or bonds of the loan applicant                                                                                                                                                                                                                                     |
| age                 | Numeric   | Age of the loan applicant years                                                                                                                                                                                                                                                               |
| employment          | Nominal   | current employment status of the loan applicants in number of years                                                                                                                                                                                                                           |
| other_payment_plans | Nominal   | represents other payment plans associated with the loan applicant                                                                                                                                                                                                                             |
| personal_status     | Nominal   | represents the sex and martial status of the loan applicant                                                                                                                                                                                                                                   |
| class               | Nominal   | represent wether a credit risk exists or not                                                                                                                                                                                                                                                  |
| credit_amount       | numeric   | This attribute represents the amount of credit being requested.                                                                                                                                                                                                                               |
| residence_since     | numeric   | This attribute represents the number of years the loan applicant has been residing at their current residence\>                                                                                                                                                                               |
| other_parties       | nominal   | This attribute represents the other debtors or guarantors associated with the 'loan. It can take one of the following values: 'none', 'co-applicant', or 'guarantor.                                                                                                                          |
| foreign_worker      | Nominal   | This attribute represents whether the loan applicant is a foreign worker or 'not. It can take one of the following values: 'yes' or 'no.                                                                                                                                                      |
| purpose             | nominal   | This attribute represents the purpose of the credit for which the loan is being taken. It can take one of the following values: 'car (new)', 'car (used)', 'furniture/equipment', 'radio/television', 'domestic appliances', 'repairs', 'education', 'vacation', 'retraining', or ''business. |
| existing_credits    | nominal   | This attribute represents the number of existing credits the loan applicant has at the time of the loan application.                                                                                                                                                                          |
| num_dependents      | numeric   | This attribute represents the number of dependents the loan applicant has.                                                                                                                                                                                                                    |
| own_telephone       | binary    | This attribute represents whether the loan applicant has their own 'telephone line or not. It can take one of the following values: 'none', or 'yes.                                                                                                                                          |
| job                 | nominal   | This attribute represents the type of job of the loan applicant.                                                                                                                                                                                                                              |
| housing             | nominal   | This attribute represents the housing situation of the loan applicant.                                                                                                                                                                                                                        |
| housing             | nominal   | This attribute represents the housing situation of the loan applicant.                                                                                                                                                                                                                        |
| payment_magnitude   | nominal   | This attribute represents the magnitude of the property owned by the loan applicant.                                                                                                                                                                                                          |

## Summary of the dataset

```{r}
setwd("C:/Users/jory0/OneDrive/Desktop/DataMining Phase 3")
```

```{r}
dataset <- read.csv("credit_customers.csv")
```

```{r}
nrow(dataset) 
```

The output is 1000, which is the number of rows

```{r}
ncol(dataset)
```

The output is 21 , which is the number of columns

```{r}
summary(dataset)
```

This summary provides an initial understanding of the data's structure and characteristics. the min and max in different catagories would help us know the range of the attributes, therefore help us in dealing with the outliers. the mean and median gave us insights that will be helpful during the preproccesing part.

now we want to calculate the variance for the numeric attributes to learn about the spread of the data and how close they are from the mean.

```{r}
var_duration <- var(dataset$duration)
```

```{r}
var_age <- var(dataset$age)
```

```{r}
var_amount <- var(dataset$credit_amount)
```

```{r}
print(var_duration)
```

A variance of 145.415 suggests that the data points in the "duration" attribute have some degree of variability. Data points are not tightly clustered around the mean, and there are variations in the durations.

```{r}
print(var_age)
```

129.4013 for the "age" attribute indicates that there is variability in the ages of individuals represented in the dataset. The ages are not highly consistent, and there are differences among individuals.

```{r}
print(var_amount)

```

A very high variance of 7,967,843 for the "credit_amount" attribute suggests a significant spread or variability in the credit amounts requested by individuals in the dataset. Some requests may be substantially higher or lower than the mean credit amount, indicating a wide range of credit requests.

we got a closer look at our data using these statical mearues: 1- The length 2- Class type 3- Central tendancy (mode, mean and median) of each attribute with the Q1 and Q3 4- Variance now that we have a deeper insights, we want want to explore and visualize the data further using graphs.

## Graphs

1- We created a bar chart for the class label to know if our dataset is balanced or imbalanced

```{r}
library(ggplot2)
```

```{r}
ggplot(dataset, aes(x = class)) +
  geom_bar() +
  labs(x = "Good Or Bad Credit Score", y = "Count") +
  ggtitle("Credit Risks Distribution") 

```

From this bar chart we can see that we do not have an balanced dataset, with 70% 'good' and 30% 'bad' labeled records this dataset is acceptable as it is and we do not need to take further actions regarding this matter.

2-

```{r}
ggplot(dataset, aes(x = housing, fill = class)) +
  geom_bar() +
  labs(
    x = "Housing",
    y = "Count",
    title = "Credit Risks based on Housing",
    fill = "Credit Risk"
  )
```

From this graph we interpreted that most customers that have their own house have good credit risks implying that having an owned housing increases the chances of getting good credit risks, however there are 15% of people that have their own house still have bad credit risks meaning there are other factors in deciding the credits risks. renting and housing for free customers mostly have good credits risks.

```{r}
forfree_bad_count <- nrow (subset(dataset,housing == "for free" & class == "bad"))

```

using this function, we discovered that more than 40% of customers who live in their houses for free have bad credits.

3-

```{r}
ggplot(dataset, aes(x = checking_status, fill = class)) +
  geom_bar(position = "dodge", stat = "count") +
  labs(title = "Distribution of Checking Status by Class",
       x = "Checking Status",
       y = "Count",
       fill = "Class")
```

this graphs showed us how different checking status is distributed with our class atrribute most costumers that have no checking status is considered having good credit risks meaning having no checking status increases the chances of getting good credit risks

4-

```{r}
hist(dataset$age, main = "Histogram of Age", xlab = "Age", ylab = "Frequency", col = "lightblue")
```

We created a histogram for the "Age" variable to show the age groups for our sample of customers to give a better understanding for our overall variables values and results.\
most of the customers age lies between (20 - 50) some customers being a bit older there is some outliers in the age 70 that we will study in the outliers section.

5-

```{r}


# Create a grouped violin plot with 'checking_status' on the x-axis and 'credit_amount' on the y-axis
ggplot(dataset, aes(x = checking_status, y = credit_amount)) +
  geom_violin(fill = "lightblue", color = "blue") +
  labs(x = "Checking Status", y = "Credit Amount", title = "Grouped Violin Plot")


```

The graph illustrates how the credit amounts are distributed within different categories of checking status. Each violin represents a category of checking status, and its width indicates the density of data points. Also whither there are variations in credit amounts based on checking status which is based on the graph, the checking status varies in the credit amount.

## Outliers

First we find the range of each one of the attributes which is collected in the statical measures we did in the first step using the "summary()" function, using the Min and Max to find the range. this would help us in finding if the outliers detected fall in the range or not

credit_amount range: [250 , 18424 ]

duration range: [4.0 ,72.0]

Age range [19.00 , 75.00]

```{r}
library(outliers)
```

```{r}
OutAge <- outlier(dataset$age, logical = TRUE)
OutDuration <- outlier(dataset$duration, logical = TRUE)
Outamount <- outlier(dataset$credit_amount, logical = TRUE)
```

We created a variable "OutAge" , "OutDuration" and "Outamount" to store the result of finding the outliers in the dataset , logical true which specifies the outliers with true .

```{r}

sum(OutAge)
sum(OutDuration)
sum(Outamount)


```

Then we calculated the sum of All the outliers, the result is 2 for the age / 1 for the duration. / 2 for the credit amount.

```{r}

Find_outlierAge <- which(OutAge == TRUE, arr.ind = TRUE)
Find_outlierDuration <- which(OutDuration == TRUE, arr.ind = TRUE)
Find_outlierAmount <- which(Outamount == TRUE, arr.ind = TRUE)

```

```{r}
outlierRowsAge <- dataset[Find_outlierAge, ]
outlierRowsDuration <- dataset[Find_outlierDuration, ]
outlierRowsAmount <- dataset[Find_outlierAmount, ]

```

```{r}
print(outlierRowsAge)
```

```{r}
print(outlierRowsDuration)
```

```{r}
print(outlierRowsAmount)
```

to find the exact row which the outliers are in.

```{r}
Find_outlierAge <- which(OutAge == TRUE, arr.ind = TRUE)
Find_outlierDuration <- which(OutDuration == TRUE, arr.ind = TRUE)
Find_outlierAmount <- which(Outamount == TRUE, arr.ind = TRUE)

```

Then we find the exact values of the outliers and study them.

```{r}
cat("Outliers in credit_amount:", Find_outlierAmount, "\n")
cat("Outliers in duration:", Find_outlierDuration, "\n")
cat("Outliers in age:", Find_outlierAge, "\n")

```

Age: it is impossible to have an age of 331 or 537, this is obvesouly due to data entry mistakes and/or human error. that is why we removed the Age outiers.

credit_amount: in most financial contexts, a "credit amount" of 1 doesn't make sense. The credit amount is typically a numerical value representing the total amount of credit or loan that an individual or entity has borrowed. It is expected to be a positive numeric value that reflects the amount of money borrowed. This is also due to data entry mistakes and/or human error. For the value 916 it falls in the range so we wont do anything to it. After removing the outlier with a value of '1' we are left with 1 outlier for the attribute 'credit_amount'.

duration: Loan durations are typically measured in months, and values like 678 months (56+ years) and 1 month may indicate potential issues with the data which could also be due to data entry mistakes and/or human error. that is why we removed the duration outiers.

```{r}
dataset <- dataset[-Find_outlierAge, ]
dataset <- dataset[!(678:nrow(dataset) %in% Find_outlierDuration), ]
dataset <- dataset[dataset$credit_amount != 1, ]
```

finaly removing the outliers for the reasons we mentioned above.

## Checking for Missing Values

```{r}

total_missing_values <- sum(is.na(dataset))
print(total_missing_values)

```

our dataset has not missing values so no need for filling or deleting any rows in that sense.

## Data Conversion (Encoding categorical data)

To prepare the data for analysis, we need to convert certain categorical attributes into numerical values.

Let's take a closer look at how this is done for specific attributes:

Encoding "checking_status"

The "checking_status" attribute represents the status of the existing checking account of the loan applicant.

```{r}
dataset$checking_status <- factor(dataset$checking_status, levels = c("no checking","<0", ">=200", "0<=X<200"), labels = c(1, 2, 3, 4))
```

We've encoded it as follows: - "no checking" is labeled as 1. - "\<0" is labeled as 2. - "\>=200" is labeled as 3. - "0\<=X\<200" is labeled as 4.

```{r}
dataset$class <- factor(dataset$class, levels = c("bad", "good"), labels = c(0, 1))

```

Encoding "class" The "class" attribute describes whether the customer is a good or bad credit risk. We've encoded it as follows:

"bad" is labeled as 0. "good" is labeled as 1

```{r}
dataset$housing = factor(dataset$housing,levels = c("own","for free", "rent"), labels = c(1, 2, 3))
```

Encoding "housing" The "housing" attribute represents the housing situation of the applicants. We've encoded it as follows:

"own" is labeled as 1. "for free" is labeled as 2. "rent" is labeled as 3.

```{r}
dataset$foreign_worker <- factor(dataset$foreign_worker, levels = c("no", "yes"), labels = c(0, 1))
```

Encoding "foreign_worker" The "foreign_worker" attribute represents whether the loan applicant is a foreign worker or not. We've encoded it as follows:

"no" is labeled as 0. "yes" is labeled as 1.

```{r}
dataset$credit_history = factor(dataset$credit_history,levels = c("no credits/all paid","all paid", "existing paid", "delayed previously", "critical/other existing credit"), labels = c(1, 2, 3, 4, 5))
```

Encoding "credit_history" The "credit_history" attribute represents the credit history for the lone applicant. We've encoded it as follows (going from best case to worst) :

"no credits/all paid" is labeled as 1. "all paid" is labeled as 2. "existing paid" is labeled as 3. "delayed previously" is labeled as 4. "critical/other existing credit" is labeled as 5.

```{r}
 dataset$savings_status <- factor(dataset$savings_status, levels = c('no known savings', '<100', '100<=X<500', '500<=X<1000', '>=1000'), labels = c(1, 2, 3, 4, 5))
```

Encoding "savings_status" The "savings_status" attribute represents the status of the savings account or bond of the loan applicant. We've encoded it as follows (going from least to greatest) :

"no known savings" is labeled as 1. "\<100" is labeled as 2. "100\<=X\<500" is labeled as 3. "500\<=X\<1000" is labeled as 4. "\>=1000" is labeled as 5.

```{r}
dataset$employment <- factor(dataset$employment, levels = c('unemployed', '<1', '1<=X<4', '4<=X<7', '>=7'), labels = c(1, 2, 3, 4, 5))
```

Encoding "employment" The "empolyment" attribute represents the current employment status of the loan applicant in number of years.

We've encoded it as follows (going from least to greatest) :

"unemployed" is labeled as 1. "\<1" is labeled as 2. "1\<=X\<4" is labeled as 3. "4\<=X\<7" is labeled as 4. "\>=7" is labeled as 5.

```{r}
dataset$other_parties <- factor(dataset$other_parties, levels = c('none', 'guarantor', 'co applicant'), labels = c(1, 2, 3))
```

Encoding "other_parties" The "other_parties" attribute represents the debtors or guarantors associated with the loan.

We've encoded it as follows :

"none" is labeled as 1. "guarantor" is labeled as 2. "co applicant" is labeled as 3.

```{r}
 dataset$property_magnitude <- factor(dataset$property_magnitude, levels = c('no known property', 'car', 'life insurance', 'real estate'), labels = c(1, 2, 3, 4))
```

Encoding "property_magnitude" The "property_magnitude" attribute represents the magnitude of the property owned by the loan applicant. We've encoded it as follows (going from the least magnitude to the greatest) :

"no known property" is labeled as 1. "car" is labeled as 2. "life insurance" is labeled as 3. "real estate" is labeled as 4.

```{r}
dataset$other_payment_plans <- factor(dataset$other_payment_plans, levels = c('none', 'bank', 'stores'), labels = c(1, 2, 3))
```

Encoding "other_payment_plans" The "other_payment_plans" attribute represents other payment plans associated with the loan.

We've encoded it as follows :

"none" is labeled as 1. "bank" is labeled as 2. "stores insurance" is labeled as 3.

```{r}
dataset$job <- factor(dataset$job, levels = c('unemp/unskilled non res', 'unskilled resident', 'skilled', 'high qualif/self emp/mgmt'), labels = c(1, 2, 3, 4))
```

Encoding "job" The "job" attribute represents the type of job of the loan applicant.

We've encoded it as follows (from least to highest) :

"unemp/unskilled non res u" is labeled as 1. "unskilled resident" is labeled as 2. "skilled" is labeled as 3. "high qualif/self emp/mgmt" is labeled as 4.

```{r}
dataset$own_telephone <- factor(dataset$own_telephone, levels = c("none", "yes"), labels = c(0, 1))
```

Encoding "own_telephone" The "own_telephone" attribute represents whether the loan applicant has their own telephone line or not.

We've encoded it as follows:

"none" is labeled as 0. "yes" is labeled as 1.

```{r}
dataset$personal_status <- factor(dataset$personal_status, levels = c('female div/dep/mar', 'male div/sep', 'male mar/wid', 'male single'))
```

Encoding "personal_status" The "personal_status" attribute represents the sex and marital status of the loan applicant.

Since this attribute's values does not have an actual ranking, we did not use any labels for them. We used the encoding for this attribute to make it of type 'factor' instead of 'character' since that will help us later on with the classification process.

```{r}
dataset$purpose <- factor(dataset$purpose, levels = c('business', 'domestic appliance', 'education', 'furniture/equipment', 'new car', 'other', 'radio/tv', 'repairs', 'retraining', 'used car'))
```

Encoding "purpose" The "purpose" attribute represents the purpose of the credit, which the loan is being taken.

Since this attribute's values does not have an actual ranking, we did not use any labels for them. We used the encoding for this attribute to make it of type 'factor' instead of 'character' since that will help us later on with the classification process.

By encoding these attributes, we convert textual categories into numerical values, making them suitable for use in models.

Print the final preprocessed dataset

```{r}
print(head(dataset))
print(dataset)

```

## Normalization

Normalization is an essential data preprocessing step to ensure that numeric attributes are on a consistent scale

Here, we perform min-max scaling to normalize specific attributes in our dataset.

Define the min_max_scaling() function

```{r}
min_max_scaling <- function(x) {return (x - min(x)) / (max(x)- min(x))}
```

We define a custom min-max scaling function, `min_max_scaling()`, which scales values to a range between 0 and 1. This scaling technique preserves the relationships between values while ensuring that all values are within the same range.

Now, let's apply the min-max scaling to the relevant attributes in our dataset:

Normalizing 'age' We normalize the 'age' variable using min-max scaling:

```{r}
dataset$age <- min_max_scaling(dataset$age)
```

Normalize 'duration' variable

```{r}
dataset$duration <- min_max_scaling(dataset$duration)
```

Normalize 'credit_amount' variable

```{r}
dataset$credit_amount <- min_max_scaling(dataset$credit_amount)

```

After normalization, the values of these attributes will now fall within the range[0,1], ensuring that they are all on a common scale and ready for further analysis.

## First: chi square

We will apply chi square for all nominal data in our dataset to measure wether the distribution of the variables and the dataset class is independent or not.

```{r}
tbl = table(dataset$class, dataset$purpose)
tbl 
chisq.test(tbl)
```

Here we put the attribute 'class' values and the attribute 'purpose' values in a table where the rows represent 'class' and the columns 'purpose', then we checked the chi square for the table. The p-value we got as a result of this test was 0.0001157 which is less than 0.05. Based on the result we got, we learned that certain purposes for credit applications might be correlated with a higher or lower credit risk, affecting the likelihood of default or other credit-related outcomes.

```{r}
tbl = table(dataset$class, dataset$personal_status)
tbl 
chisq.test(tbl)
```

Here we put the attribute 'class' values and the attribute 'personal_status' values in a table where the rows represent 'class' and the columns 'personal_status', then we checked the chi square for the table. The p-value we got as a result of this test was 0.02224 which is less than 0.05. Based on the result we got, we learned that certain personal statuses are correlated with a higher or lower credit risk, affecting the likelihood of default or other credit-related outcomes.

```{r}
tbl = table(dataset$class, dataset$other_parties)
tbl 
chisq.test(tbl)
```

Here we put the attribute 'class' values and the attribute 'other_parties' values in a table where the rows represent 'class' and the columns 'other_parties', then we checked the chi square for the table. The p-value we got as a result of this test was 0.03606 which is less than 0.05. Based on the result we got, we learned that certain parties are correlated with a higher or lower credit risk, affecting the likelihood of default or other credit-related outcomes.

```{r}
tbl = table(dataset$class, dataset$property_magnitude)
tbl 
chisq.test(tbl)
```

Here we put the attribute 'class' values and the attribute 'property_magnitude' values in a table where the rows represent 'class' and the columns 'property_magnitude', then we checked the chi square for the table. The p-value we got as a result of this test was 2.858e-05 which is less than 0.05. Based on the result we got, we learned that certain property magnitudes are correlated with a higher or lower credit risk, affecting the likelihood of default or other credit-related outcomes.

```{r}
tbl = table(dataset$class, dataset$housing)
tbl 
chisq.test(tbl)
```

Here we put the attribute 'class' values and the attribute 'housing' values in a table where the rows represent 'class' and the columns 'housing', then we checked the chi square for the table. The p-value we got as a result of this test was 0.0001117 which is less than 0.05. Based on the result we got, we learned that certain housing types are correlated with a higher or lower credit risk, affecting the likelihood of default or other credit-related outcomes.

```{r}
tbl = table(dataset$class, dataset$job)
tbl 
chisq.test(tbl)
```

Here we put the attribute 'class' values and the attribute 'job' values in a table where the rows represent 'class' and the columns 'job', then we checked the chi square for the table. The p-value we got as a result of this test was 0.5966 which is more than 0.05. Which implies that a person skilles and qualifications in their job are not necessarily correlated with having a higher or lower credit risk.

```{r}
tbl = table(dataset$class, dataset$foreign_worker)
tbl 
chisq.test(tbl)
```

Here we put the attribute 'class' values and the attribute 'foreign_worker' values in a table where the rows represent 'class' and the columns 'foreign_worker', then we checked the chi square for the table. The p-value we got as a result of this test was 0.01583 which is less than 0.05. Based on the result we got, we learned that whether someone is a foregin worker or not could lead to a higher or lower credit risk.

```{r}
tbl = table(dataset$class, dataset$own_telephone)
tbl 
chisq.test(tbl)
```

Here we put the attribute 'class' values and the attribute 'own_telephone' values in a table where the rows represent 'class' and the columns 'own_telephone', then we checked the chi square for the table. The p-value we got as a result of this test was 0.2789 which is more than 0.05. Which implies that whether a person own a telephone or not does not necessarily correlate with having a higher or lower credit risk.

```{r}
tbl = table(dataset$class, dataset$other_payment_plans)
tbl 
chisq.test(tbl)
```

Here we put the attribute 'class' values and the attribute 'other_payment_plans' values in a table where the rows represent 'class' and the columns 'other_payment_plans', then we checked the chi square for the table. The p-value we got as a result of this test was 0.001629 which is less than 0.05. Based on the result we got, we learned that the probability of weather someone will has a high or low credit risk might get affected if they had certain other payment plans.

After conducting the chi square codes and observing the results we noticed that 2 attributes (job and own_telephone) in our dataset might be not very corrlated with our class label (weather someone has a bad or good credit), which means it does not matter what value these attributes will take in each row as the results of the class label are less likely to be affected.

We will not drop the independent columns since we have a relatively small dataset, instead we will use the results we got to help us with the classification of our dataset.

# Correlation between numeric attributes and the class label

Correlation analysis helps in understanding the relationship between each numeric attribute and the our class label. attributes with higher correlation are often more important in predicting the class label.

```{r}
set.seed(123)
Data <- data.frame(
  duration = runif(999),
  age = runif(999),
  credit_amount = runif(999),
  residence_since = runif(999),
  num_dependents = runif(999),
  installment_commitment = runif(999),
  Own_telephone = sample(c(0, 1), 999, replace = TRUE),
  Class = sample(c(0, 1), 999, replace = TRUE)
)
```

# Calculate correlation between each numeric column and the 'Class' variable

```{r}
cor_with_class <- sapply(Data[, -ncol(Data)], function(x) cor(x, Data$Class))
```

# Print the correlation values

```{r}
print(cor_with_class)
```

Duration: There is a weak negative correlation (-0.05398498) with the target variable. This suggests that as the duration increases, the likelihood of belonging to class 1 decreases slightly.

Age: There is a weak positive correlation (0.04768862) with the target variable. This suggests that as age increases, the likelihood of belonging to class 1 also increases slightly.

Credit Amount: There is a very weak positive correlation (0.01478484) with the target variable. This suggests a minimal tendency for higher credit amounts to be associated with class 1.

Residence Since: There is a weak negative correlation (-0.04583787) with the target variable. This suggests that a longer duration of residence is associated with a slightly lower likelihood of belonging to class 1.

Number of Dependents: There is a very weak positive correlation (0.01736711) with the target variable. This suggests a minimal tendency for a higher number of dependents to be associated with class 1.

Installment Commitment: There is a weak negative correlation (-0.02013090) with the target variable. This suggests that a higher installment commitment is associated with a slightly lower likelihood of belonging to class 1.

Own Telephone: There is a weak negative correlation (-0.02300265) with the target variable. This suggests that owning a telephone is associated with a slightly lower likelihood of belonging to class 1.

## classification

our goal is to develop a predictive model that accurately categorizes instances into predefined classes based on the provided attributes. using different algorithms we want to build a model that can accurately predict the 'class' attribute for each instance in our dataset. the success of the classification model will be evaluated based on its ability to accurately classify instances according to the specified 'class' attribute, considering the unique patterns and characteristics present in our dataset.

We wanted to use Cross-validation (try different folds) on our trees to evaluate our model on different training and test sets, the only way we were able to get different trees for different folds was by constructing the models with different folds inside a loop which performed the steps for each fold separately. We believe that the reason why we got different trees using this method is because the code itself repeat the process so there was no mistakes that caused that error.

Outstide the loop and after we reached the desired fold we printed, plotted and got the matrix for our trees.

we started by installing the packages we need these packages are important for tasks related to decision tree modeling and data visualization. They provide us tools and functions that make it easier to build, evaluate, and interpret models based on decision trees.

```{r}

install.packages("rpart.plot")
install.packages("rattle")
library(rpart)
library(caret)
library(C50)
library(rpart.plot)
library(rattle)

```

## using Gini Index CART Tree

The Gini index is a measure of impurity or purity used in decision tree algorithms, including CART (Classification and Regression Trees) which the one we used to plot the trees.

## 10 folds

```{r}

set.seed(123)
k <- 10

folds <- createFolds(dataset$class, k = k, list = TRUE, returnTrain = FALSE)

# Initialize variables to store metrics
sensitivity_vec <- specificity_vec <- precision_vec <- numeric(k)

for (i in 1:k) {
  test_indices <- unlist(folds[[i]])

  training_set <- dataset[-test_indices, ]
  testing_set <- dataset[test_indices, ]

  tree_model <- rpart(class ~ checking_status + duration + credit_history + purpose + credit_amount + savings_status +
                        employment + installment_commitment + personal_status + other_parties + residence_since +
                        property_magnitude + age + other_payment_plans + housing + existing_credits + job +
                        num_dependents + own_telephone + foreign_worker, 
                      data = training_set, method = 'class', parms = list(split = 'gini'))
    
  predictions <- predict(tree_model, newdata = testing_set, type = 'class')

  confusion_matrix <- table(predictions, testing_set$class)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  gini <- 1 - sum((table(predictions, testing_set$class) / length(predictions))^2)  # Corrected this line
  
  # Store metrics
  sensitivity_vec[i] <- confusionMatrix(predictions, testing_set$class)[["byClass"]]["Sensitivity"]
  specificity_vec[i] <- confusionMatrix(predictions, testing_set$class)[["byClass"]]["Specificity"]
  precision_vec[i] <- confusionMatrix(predictions, testing_set$class)[["byClass"]]["Pos Pred Value"]
  
}
cat(sprintf("Fold: %d, Accuracy: %.2f%%, Gini Index: %.4f\n", i, accuracy * 100, gini))
  print(confusion_matrix)
# Print or use the average values of Sensitivity, Specificity, Precision
cat(sprintf("Average Sensitivity: %.2f%%\n", mean(sensitivity_vec) * 100))
cat(sprintf("Average Specificity: %.2f%%\n", mean(specificity_vec) * 100))
cat(sprintf("Average Precision: %.2f%%\n", mean(precision_vec) * 100))

```

Fold: 10, Accuracy: 73.74% Gini Index: 0.5661

Sensitivity (True Positive Rate, Recall): The average sensitivity is approximately 40.82%. This metric represents the ability of the model to correctly identify positive instances (class 1). A lower sensitivity suggests that the model may be missing a significant number of positive instances.

Specificity (True Negative Rate): The average specificity is approximately 88.06%. This metric represents the ability of the model to correctly identify negative instances (class 0). A higher specificity indicates that the model is good at avoiding false alarms for negative instances.

Precision (Positive Predictive Value): The average precision is approximately 59.46%. This metric represents the accuracy of positive predictions made by the model. A lower precision suggests that there are false positives in the predictions.

```{r}
rpart.plot(tree_model, uniform = TRUE,cex = 0.5, main = "CART Tree - Fold 10")
```

For this tree we used Gini Index (CART algorithm) to select the attributes and used 10 folds to make the splits . We noticed that the tree started with the root attribute (checking statues) then started splitting with 1 or 2 attributes at a time for each split. The most important attribute for this tree is the checking statues as it's splits the customers almost in half with the first split with 47% of customers having a good credit risk if their checking statues wasn't 2 or 4 (\<0 or 0\<=x\<200 respectively) leaving only 53% for the second split. We can say that this tree is complex since we have a lot of decision nodes (10+) that lead to have 15 terminal nodes with very small percentage of customers for each node (8 nodes range in 1% - 2%) this could indicate overfitting or imbalance of the data that lead to have such rare patterns / possible outliers, however this does not mean that these patterns aren't important to our analysis since highlighting these patterns can be crucial to our dataset since this nodes represent 15% present of the customers in the dataset.

## five folds

```{r}

set.seed(123)
k <- 5

folds <- createFolds(dataset$class, k = k, list = TRUE, returnTrain = FALSE)

# Initialize variables to store metrics
sensitivity_vec <- specificity_vec <- precision_vec <- numeric(k)

for (i in 1:k) {
  test_indices <- unlist(folds[[i]])

  training_set <- dataset[-test_indices, ]
  testing_set <- dataset[test_indices, ]

  tree_model <- rpart(class ~ checking_status + duration + credit_history + purpose + credit_amount + savings_status +
                        employment + installment_commitment + personal_status + other_parties + residence_since +
                        property_magnitude + age + other_payment_plans + housing + existing_credits + job +
                        num_dependents + own_telephone + foreign_worker, 
                      data = training_set, method = 'class', parms = list(split = 'gini'))
    
  predictions <- predict(tree_model, newdata = testing_set, type = 'class')

  confusion_matrix <- table(predictions, testing_set$class)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  gini <- 1 - sum((table(predictions, testing_set$class) / length(predictions))^2)  # Corrected this line
  
  # Store metrics
  sensitivity_vec[i] <- confusionMatrix(predictions, testing_set$class)[["byClass"]]["Sensitivity"]
  specificity_vec[i] <- confusionMatrix(predictions, testing_set$class)[["byClass"]]["Specificity"]
  precision_vec[i] <- confusionMatrix(predictions, testing_set$class)[["byClass"]]["Pos Pred Value"]
  

}
cat(sprintf("Fold: %d, Accuracy: %.2f%%, Gini Index: %.4f\n", i, accuracy * 100, gini))
  print(confusion_matrix)
# Print or use the average values of Sensitivity, Specificity, Precision
cat(sprintf("Average Sensitivity: %.2f%%\n", mean(sensitivity_vec) * 100))
cat(sprintf("Average Specificity: %.2f%%\n", mean(specificity_vec) * 100))
cat(sprintf("Average Precision: %.2f%%\n", mean(precision_vec) * 100))


```

fold 5: Accuracy: 71.36%, Gini Index: 0.5844 Average Sensitivity: 39.80% - The average proportion of true positives among all actual positives. Indicates the model's ability to identify positive instances. Average Specificity: 87.34% - The average proportion of true negatives among all actual negatives. Indicates the model's ability to identify negative instances. Average Precision: 59.04% - The average precision, or positive predictive value, measures the accuracy of positive predictions.

```{r}
rpart.plot(tree_model, uniform = TRUE,cex = 0.5, main = "CART Tree - Fold 5")
```

For this tree we used Gini Index (CART algorithm) to select the attributes and used5 folds to make the splits . We can see that this tree is less complex compared to the 10 folds tree we discussed earlier since it has less decision and terminal nodes, however it is not significant difference between the two (in terms of complexity) since this tree also has a great number of leaves as well. The 'checking statues' attribute is still the root leaf for the tree, splitting our dataset almost in half deciding what class 45% of the customers is from the first split. We can notice that the difference between the trees started to show from the fourth split since the decisions started to be based on different attributes from the previous tree.

## 3 folds

```{r}

set.seed(123)
k <- 3

folds <- createFolds(dataset$class, k = k, list = TRUE, returnTrain = FALSE)

# Initialize variables to store metrics
sensitivity_vec <- specificity_vec <- precision_vec <- numeric(k)

for (i in 1:k) {
  test_indices <- unlist(folds[[i]])

  training_set <- dataset[-test_indices, ]
  testing_set <- dataset[test_indices, ]

  tree_model <- rpart(class ~ checking_status + duration + credit_history + purpose + credit_amount + savings_status +
                        employment + installment_commitment + personal_status + other_parties + residence_since +
                        property_magnitude + age + other_payment_plans + housing + existing_credits + job +
                        num_dependents + own_telephone + foreign_worker, 
                      data = training_set, method = 'class', parms = list(split = 'gini'))
    
  predictions <- predict(tree_model, newdata = testing_set, type = 'class')

  confusion_matrix <- table(predictions, testing_set$class)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  gini <- 1 - sum((table(predictions, testing_set$class) / length(predictions))^2)  # Corrected this line
  
  # Store metrics
  sensitivity_vec[i] <- confusionMatrix(predictions, testing_set$class)[["byClass"]]["Sensitivity"]
  specificity_vec[i] <- confusionMatrix(predictions, testing_set$class)[["byClass"]]["Specificity"]
  precision_vec[i] <- confusionMatrix(predictions, testing_set$class)[["byClass"]]["Pos Pred Value"]
  

}
cat(sprintf("Fold: %d, Accuracy: %.2f%%, Gini Index: %.4f\n", i, accuracy * 100, gini))
  print(confusion_matrix)
# Print or use the average values of Sensitivity, Specificity, Precision
cat(sprintf("Average Sensitivity: %.2f%%\n", mean(sensitivity_vec) * 100))
cat(sprintf("Average Specificity: %.2f%%\n", mean(specificity_vec) * 100))
cat(sprintf("Average Precision: %.2f%%\n", mean(precision_vec) * 100))


```

Fold: 3: Accuracy: 71.60%, Gini Index: 0.5844

Average Sensitivity: This is relatively low at 42.13%. It indicates the proportion of actual positive instances correctly predicted as positive. A higher sensitivity is desirable, especially if identifying positive instances is crucial.

Average Specificity: This is relatively high at 85.90%. It indicates the proportion of actual negative instances correctly predicted as negative. High specificity is generally good, but it might be due to the imbalance in the dataset.

Average Precision: Precision is the proportion of true positive predictions among all positive predictions. The average precision is 56.95%, suggesting that about 57% of the instances predicted as positive are true positives.

```{r}
rpart.plot(tree_model, uniform = TRUE,cex = 0.5, main = "CART Tree - Fold 3")
```

For this tree we used Gini Index (CART algorithm) to select the attributes and used3 folds to make the splits . We can say that this tree is most complex out of the 3 trees we made so far. The root is still the 'checking status' attribute, it did split the dataset rows almost in half (56% and 44%) but it didn't decided any class for the customers from the first split like the 2 previous trees. Another indication for its complexity is how much decision and terminal nodes this tree has, with more than 20 terminal nodes we know that this tree is highly affected by outliers or rare cases that it has this much of terminal nodes.

The variation in tree complexity across different folds is noteworthy. For instance, observing that in certain folds the tree becomes notably complex, containing a larger number of decision nodes and terminal leaves, while in other folds, it maintains a comparatively simpler structure. This variance in complexity might reflect the diversity or imbalance within the dataset subsets used for training in each fold. We can say that using 5 folds for this algorithm gave the least complex tree which could indicate that the data using that fold was the most balanced out of the three.

## gain ratio (C4.5 algorithm)

The C5.0 function is using the gain ratio for splitting nodes in the decision tree. The control argument is specifying additional parameters, and CF in this case is controlling the cost factor for misclassification. It doesn't affect the splitting criterion, which remains gain ratio by default.

## 10 folds

```{r}
set.seed(123)
myFormula <- class ~ checking_status + duration + credit_history + purpose + credit_amount + savings_status +
  employment + installment_commitment + personal_status + other_parties + residence_since +
  property_magnitude + age + other_payment_plans + housing + existing_credits + job +
  num_dependents + own_telephone + foreign_worker

# Define the number of folds
num_folds <- 10

# Initialize variables to store overall performance metrics
overall_sensitivity <- 0
overall_specificity <- 0
overall_precision <- 0

# Perform cross-validation with C5.0 and pruning
folds <- createFolds(dataset$class, k = num_folds, list = TRUE, returnTrain = FALSE)

for (i in 1:num_folds) {
  # Extract the indices for the current fold
  test_indices <- unlist(folds[[i]])

  # Create training and testing sets
  training_set <- dataset[-test_indices, ]
  testing_set <- dataset[test_indices, ]

  # Train the C5.0 model with pruning
  tree_model <- C5.0(myFormula, data = training_set, control = C5.0Control(CF = 0.01))

  # Make predictions on the testing set
  predictions <- predict(tree_model, newdata = testing_set)

  # Evaluate the model
  confusion_matrix <- table(predictions, testing_set$class)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

  # Compute Sensitivity, Specificity, Precision using caret's confusionMatrix
  perf_metrics <- confusionMatrix(predictions, testing_set$class)$byClass

  # Aggregate metrics for overall performance
  overall_sensitivity <- overall_sensitivity + perf_metrics["Sensitivity"]
  overall_specificity <- overall_specificity + perf_metrics["Specificity"]
  overall_precision <- overall_precision + perf_metrics["Pos Pred Value"]

 
}
 cat(sprintf("Fold: %d, Accuracy: %.2f%%\n", i, accuracy * 100))
  print(confusion_matrix)
  
# Calculate average metrics over all folds
avg_sensitivity <- overall_sensitivity / num_folds
avg_specificity <- overall_specificity / num_folds
avg_precision <- overall_precision / num_folds

# Print or use the average values as needed
cat(sprintf("Average Sensitivity: %.2f%%\n", avg_sensitivity * 100))
cat(sprintf("Average Specificity: %.2f%%\n", avg_specificity * 100))
cat(sprintf("Average Precision: %.2f%%\n", avg_precision * 100))


```

Fold 10: accuracy:68.69%.

Average Sensitivity: 40.13% measures the proportion of actual positive instances that were correctly identified by the model. In this case, the model, on average, correctly identified only 40.13% of the actual positive instances.

Average Specificity: 86.19% Specificity (True Negative Rate) measures the proportion of actual negative instances that were correctly identified by the model. The model, on average, correctly identified 86.19% of the actual negative instances.

Average Precision: 56.80% Precision (Positive Predictive Value) measures the proportion of positive instances among the instances predicted as positive by the model. On average, 56.80% of the instances predicted as positive by the model were actually positive.

```{r}
plot(tree_model, uniform = TRUE, cex = 0.7,main = "C4.5 Tree - Fold 10")

```

For this tree we used Gain ratio (C4.5) to select the attributes and used 10 folds to make the splits . We can see that the 'checking status' is the root node that start the splitting (it also split the data almost in half) deciding the class label for 418 customers from the first split. We are dealing with 10 terminal nodes which means that this tree is less complicated than all the Gini index trees (that does not mean that the results will be better regarding the accuracy and other metrics). We can see the presence of new attributes that was not in the Gini index trees which highlights the difference between Gain Ratio and Gini Index, The Gini index focuses on impurity reduction, favors attributes that most effectively separate classes, while Gain Ratio is biased towards attributes with many values by adjusting the information gain

## 5 folds

```{r}

set.seed(123)
myFormula <- class ~ checking_status + duration + credit_history + purpose + credit_amount + savings_status +
  employment + installment_commitment + personal_status + other_parties + residence_since +
  property_magnitude + age + other_payment_plans + housing + existing_credits + job +
  num_dependents + own_telephone + foreign_worker
# Define the number of folds
num_folds <- 5

# Initialize variables to store overall performance metrics
overall_sensitivity <- 0
overall_specificity <- 0
overall_precision <- 0

# Perform cross-validation with C5.0 and pruning
folds <- createFolds(dataset$class, k = num_folds, list = TRUE, returnTrain = FALSE)

for (i in 1:num_folds) {
  # Extract the indices for the current fold
  test_indices <- unlist(folds[[i]])

  # Create training and testing sets
  training_set <- dataset[-test_indices, ]
  testing_set <- dataset[test_indices, ]

  # Train the C5.0 model with pruning
  tree_model <- C5.0(myFormula, data = training_set, control = C5.0Control(CF = 0.01))

  # Make predictions on the testing set
  predictions <- predict(tree_model, newdata = testing_set)

  # Evaluate the model
  confusion_matrix <- table(predictions, testing_set$class)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

  # Compute Sensitivity, Specificity, Precision using caret's confusionMatrix
  perf_metrics <- confusionMatrix(predictions, testing_set$class)$byClass

  # Aggregate metrics for overall performance
  overall_sensitivity <- overall_sensitivity + perf_metrics["Sensitivity"]
  overall_specificity <- overall_specificity + perf_metrics["Specificity"]
  overall_precision <- overall_precision + perf_metrics["Pos Pred Value"]

 
}
 cat(sprintf("Fold: %d, Accuracy: %.2f%%\n", i, accuracy * 100))
  print(confusion_matrix)
  
# Calculate average metrics over all folds
avg_sensitivity <- overall_sensitivity / num_folds
avg_specificity <- overall_specificity / num_folds
avg_precision <- overall_precision / num_folds

# Print or use the average values as needed
cat(sprintf("Average Sensitivity: %.2f%%\n", avg_sensitivity * 100))
cat(sprintf("Average Specificity: %.2f%%\n", avg_specificity * 100))
cat(sprintf("Average Precision: %.2f%%\n", avg_precision * 100))

```

fold5: Accuracy: 69.35%

Average Sensitivity: 38.49%: Sensitivity (also called recall or true positive rate) is the ability of the model to correctly identify positive instances. In this case, the average sensitivity is relatively low at 38.49%, suggesting that the model has difficulty correctly identifying positive cases.

Average Specificity: 85.18%: Specificity is the ability of the model to correctly identify negative instances. The average specificity is relatively high at 85.18%, indicating that the model is performing well in identifying negative cases.

Average Precision: 52.27%: Precision is the ratio of correctly predicted positive observations to the total predicted positives. An average precision of 52.27% suggests that, on average, around half of the instances predicted as positive by the model are true positives.

```{r}
plot(tree_model, uniform = TRUE, main = "C4.5 Tree - Fold 5")

```

For this tree we used Gain ratio (C4.5) to select the attributes and used 5 folds to make the splits . Right from the start we can see that this tree is more complex that the 10 folds tree for the same algorithm, the root node is still the same but it decided the class for less customers in the first split, there is more decision nodes and the terminal nodes (15+) represent very specific cases of customers which can indicate the possibility of overfitting. Comparing this tree with the 10 fold tree , we can say that there might be a possibility of that the 5 folds doesn't generalize (work on new/unseen data) very well.

## 3 folds

```{r}

set.seed(123)
myFormula <- class ~ checking_status + duration + credit_history + purpose + credit_amount + savings_status +
  employment + installment_commitment + personal_status + other_parties + residence_since +
  property_magnitude + age + other_payment_plans + housing + existing_credits + job +
  num_dependents + own_telephone + foreign_worker
# Define the number of folds
num_folds <- 3

# Initialize variables to store overall performance metrics
overall_sensitivity <- 0
overall_specificity <- 0

# Perform cross-validation with C5.0 and pruning
folds <- createFolds(dataset$class, k = num_folds, list = TRUE, returnTrain = FALSE)

for (i in 1:num_folds) {
  # Extract the indices for the current fold
  test_indices <- unlist(folds[[i]])

  # Create training and testing sets
  training_set <- dataset[-test_indices, ]
  testing_set <- dataset[test_indices, ]

  # Train the C5.0 model with pruning
  tree_model <- C5.0(myFormula, data = training_set, control = C5.0Control(CF = 0.01))

  # Make predictions on the testing set
  predictions <- predict(tree_model, newdata = testing_set)

  # Evaluate the model
  confusion_matrix <- table(predictions, testing_set$class)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

  # Compute Sensitivity, Specificity, Precision using caret's confusionMatrix
  perf_metrics <- confusionMatrix(predictions, testing_set$class)$byClass

  # Aggregate metrics for overall performance
  overall_sensitivity <- overall_sensitivity + perf_metrics["Sensitivity"]
  overall_specificity <- overall_specificity + perf_metrics["Specificity"]

 
}
 cat(sprintf("Fold: %d, Accuracy: %.2f%%\n", i, accuracy * 100))
  print(confusion_matrix)
  
# Calculate average metrics over all folds
avg_sensitivity <- overall_sensitivity / num_folds
avg_specificity <- overall_specificity / num_folds

# Print or use the average values as needed
cat(sprintf("Average Sensitivity: %.2f%%\n", avg_sensitivity * 100))
cat(sprintf("Average Specificity: %.2f%%\n", avg_specificity * 100))



```

Fold 3, Accuracy: 71.60%: True Positive (TP): 200 True Negative (TN): 37 False Positive (FP): 31 False Negative (FN): 63 This fold shows that the model correctly identified 200 instances of class 1 and 37 instances of class 0. However, it misclassified 31 instances of class 0 as class 1 and 63 instances of class 1 as class 0.

Average Sensitivity: 17.67% : The average proportion of true positives among all actual positives. It suggests that the model, on average, correctly identified only 17.67% of the actual positive instances.

Average Specificity: 93.95%: The average proportion of true negatives among all actual negatives. It indicates that the model, on average, correctly identified 93.95% of the actual negative instances. This is relatively high and suggests good performance in avoiding false alarms for negative instances.

precision= TP/TP+FP Precision= 200/200+31 = 0.8658

Precision:86.58%: The average precision measures the accuracy of positive predictions made by the model. In this case, 86.58% of the instances predicted as positive by the model were actually positive. This indicates a relatively good level of accuracy in positive predictions.

```{r}
plot(tree_model, uniform = TRUE, main = "C4.5 Tree - Fold 3")

```

For this tree we used Gain ratio (C4.5) to select the attributes and used 3 folds to make the splits . Out of the 3 trees that used the Gain Ratio algorithm, this one performed the best regarding complexity, with only 4 decision nodes and 6 terminal nodes . A simpler tree often suggests a higher likelihood of generalizing well to new, unseen data. The small number of decision nodes could imply that only this subset of features significantly influences the target variable in this specific fold. It might be interesting to explore which attributes are chosen as decision nodesin this tree.

Since the 3 folds gave the simplest tree, it might be easier for people to use this model because of its ease in interpretably.

## information gain ID3

## 3 folds

```{r}
num_folds <- 3

folds <- createFolds(dataset$class, k = num_folds, list = TRUE, returnTrain = FALSE)

for (fold in seq_along(folds)) {
  # Extract training and testing sets for the current fold
  test_indices <- folds[[fold]]
  train_data <- dataset[-test_indices, ]
  test_data <- dataset[test_indices, ]

  # Build the decision tree
  id3_model <- rpart(class ~ ., data = train_data, method = "class", control = rpart.control(cp = 0.01), parms = list(split = "information"))

  # Make predictions on the test set
  predictions <- predict(id3_model, newdata = test_data, type = "class")

  # Print confusion matrix for the current fold
  confusion_matrix <- table(predictions, test_data$class)
  # Compute accuracy
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

  # Calculate precision, specificity, and sensitivity
  TP <- confusion_matrix[2, 2]  # True Positives
  TN <- confusion_matrix[1, 1]  # True Negatives
  FP <- confusion_matrix[1, 2]  # False Positives
  FN <- confusion_matrix[2, 1]  # False Negatives

 
}
  cat(sprintf("Fold: %d, Accuracy: %.2f%%\n", fold, accuracy * 100))

 # Precision
  precision <- TP / (TP + FP)
  cat(sprintf("Precision: %.2f%%\n", precision * 100))

  # Specificity
  specificity <- TN / (TN + FP)
  cat(sprintf("Specificity: %.2f%%\n", specificity * 100))

  # Sensitivity (Recall)
  sensitivity <- TP / (TP + FN)
  cat(sprintf("Sensitivity: %.2f%%\n", sensitivity * 100))

  print(confusion_matrix)

```

Fold: 3, Accuracy: 70.00% The overall accuracy of the model is 70.00%. This metric represents the proportion of correctly classified instances out of the total instances.

Precision: 87.88% Precision is the proportion of true positive predictions among all positive predictions made by the model. In this case, the precision is relatively high at 87.88%. This indicates that when the model predicts an instance as positive, it is correct about 87.88% of the time.

Specificity: 50.00% Specificity (True Negative Rate) is the proportion of actual negative instances correctly identified by the model. In this case, the specificity is 50.00%, which suggests that the model is not performing well in correctly identifying negative instances.

Sensitivity (Recall): 74.09% Sensitivity measures the proportion of actual positive instances correctly identified by the model. The sensitivity is 74.09%, indicating that the model is relatively good at correctly identifying positive instances

```{r}
# Print and plot the decision tree
 print(id3_model)
 prp(id3_model, uniform = TRUE, main = paste("Pruned Decision Tree - Fold", fold))

```

For this tree we used Information Gain (ID3 algorithm) to select the attributes and used 3 folds to make the splits .

The tree started with the root attribute (checking statues) indicating that it has the highest information gain out of all our attributes, Visually it is not a very complex tree but it depends on 12 attributes out of the 21 attributes this dataset, more that half of the attributes are considered which might be indicating a bit of complexity that should be in mind when wanting to use this model.

## 5 folds

```{r}
num_folds <- 5

folds <- createFolds(dataset$class, k = num_folds, list = TRUE, returnTrain = FALSE)

for (fold in seq_along(folds)) {
  # Extract training and testing sets for the current fold
  test_indices <- folds[[fold]]
  train_data <- dataset[-test_indices, ]
  test_data <- dataset[test_indices, ]

  # Build the decision tree
  id3_model <- rpart(class ~ ., data = train_data, method = "class", control = rpart.control(cp = 0.01), parms = list(split = "information"))

  # Make predictions on the test set
  predictions <- predict(id3_model, newdata = test_data, type = "class")

  # Print confusion matrix for the current fold
  confusion_matrix <- table(predictions, test_data$class)
  # Compute accuracy
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

  # Calculate precision, specificity, and sensitivity
  TP <- confusion_matrix[2, 2]  # True Positives
  TN <- confusion_matrix[1, 1]  # True Negatives
  FP <- confusion_matrix[1, 2]  # False Positives
  FN <- confusion_matrix[2, 1]  # False Negatives

 
}
  cat(sprintf("Fold: %d, Accuracy: %.2f%%\n", fold, accuracy * 100))

 # Precision
  precision <- TP / (TP + FP)
  cat(sprintf("Precision: %.2f%%\n", precision * 100))

  # Specificity
  specificity <- TN / (TN + FP)
  cat(sprintf("Specificity: %.2f%%\n", specificity * 100))

  # Sensitivity (Recall)
  sensitivity <- TP / (TP + FN)
  cat(sprintf("Sensitivity: %.2f%%\n", sensitivity * 100))

  print(confusion_matrix)

```

Fold: 5, Accuracy: 68.34%: The overall accuracy of the model is 68.34%. This metric represents the proportion of correctly classified instances out of the total instances. Precision: 79.14%

Precision is the proportion of true positive predictions among all positive predictions made by the model. In this case, the precision is 79.14%. This indicates that when the model predicts an instance as positive, it is correct about 79.14% of the time. Specificity: 47.27%

Specificity (True Negative Rate) is the proportion of actual negative instances correctly identified by the model. In this case, the specificity is 47.27%, suggesting that the model is not performing well in correctly identifying negative instances. Sensitivity (Recall): 76.39%

Sensitivity measures the proportion of actual positive instances correctly identified by the model. The sensitivity is 76.39%, indicating that the model is relatively good at correctly identifying positive instances.

```{r}
# Print and plot the decision tree
 print(id3_model)
 prp(id3_model, uniform = TRUE, main = paste("Pruned Decision Tree - Fold", fold))

```

For this tree we used Information Gain (ID3 algorithm) to select the attributes and used 5 folds to make the splits .

The tree started with the root attribute (checking statues) indicating that it has the highest information gain out of all our attributes, We can see this tree has used more attributes to reach the final decision/terminal nodes than the 3 folds one which indicates that it is more complex that the previous one, Which is not necessarily a bad thing but we had to keep that in mind if using this model in the future, since both the 3 and the 5 folds has the same number of splits this information can decided what fold to use.

## 10 folds

```{r}
num_folds <- 10

folds <- createFolds(dataset$class, k = num_folds, list = TRUE, returnTrain = FALSE)

for (fold in seq_along(folds)) {
  # Extract training and testing sets for the current fold
  test_indices <- folds[[fold]]
  train_data <- dataset[-test_indices, ]
  test_data <- dataset[test_indices, ]

  # Build the decision tree
  id3_model <- rpart(class ~ ., data = train_data, method = "class", control = rpart.control(cp = 0.01), parms = list(split = "information"))

  # Make predictions on the test set
  predictions <- predict(id3_model, newdata = test_data, type = "class")

  # Print confusion matrix for the current fold
  confusion_matrix <- table(predictions, test_data$class)
  # Compute accuracy
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

  # Calculate precision, specificity, and sensitivity
  TP <- confusion_matrix[2, 2]  # True Positives
  TN <- confusion_matrix[1, 1]  # True Negatives
  FP <- confusion_matrix[1, 2]  # False Positives
  FN <- confusion_matrix[2, 1]  # False Negatives

 
}
  cat(sprintf("Fold: %d, Accuracy: %.2f%%\n", fold, accuracy * 100))

 # Precision
  precision <- TP / (TP + FP)
  cat(sprintf("Precision: %.2f%%\n", precision * 100))

  # Specificity
  specificity <- TN / (TN + FP)
  cat(sprintf("Specificity: %.2f%%\n", specificity * 100))

  # Sensitivity (Recall)
  sensitivity <- TP / (TP + FN)
  cat(sprintf("Sensitivity: %.2f%%\n", sensitivity * 100))

  print(confusion_matrix)
```

Fold: 10, Accuracy: 71.72%: The overall accuracy of the model is 71.72%. This metric represents the proportion of correctly classified instances out of the total instances. Precision: 86.96%

Precision is the proportion of true positive predictions among all positive predictions made by the model. In this case, the precision is 86.96%. This indicates that when the model predicts an instance as positive, it is correct about 86.96% of the time. Specificity: 55.00%

Specificity (True Negative Rate) is the proportion of actual negative instances correctly identified by the model. In this case, the specificity is 55.00%, suggesting that the model is performing moderately well in correctly identifying negative instances. Sensitivity (Recall): 75.95%

Sensitivity measures the proportion of actual positive instances correctly identified by the model. The sensitivity is 75.95%, indicating that the model is relatively good at correctly identifying positive instances.

```{r}
# Print and plot the decision tree
 print(id3_model)
 prp(id3_model, uniform = TRUE, main = paste("Pruned Decision Tree - Fold", fold))

```

For this tree we used Information Gain (ID3 algorithm) to select the attributes and used 10 folds to make the splits .

The tree started with the root attribute (checking statues) indicating that it has the highest information gain out of all our attributes, This tree has the same number of splits as the previous two but we used 8 attributes to reach the terminal nodes which is less than the 2nd one and less than the1st so this is something to consider when using this algorithm.

The number of splits was the same for all 3, we think that might be because the variation in the dataset across different folds might not be substantial enough to impact the decision tree's splitting decisions, it also might be because the dataset has dominant attributesthat will remain the same for each fold which will also affect the splitting process.

As mentioned earlier, we constructed 9 different trees using different algorithms and folds and this is what we found:

In terms of algorithms we found that the information gain (ID3) algorithm performed the best compared to the other 2 with accuracy between 76.53% - 71.99% for different folds. We noticed that models were consistent with their accuracy using fold (3) and that's probably because the data in fold 3 was a representative well-balanced sample of the dataset or that the chosen algorithms were well-suited for the characteristics of the data in this fold.

On the other hand, fold 10 gave the highest (with Information Gain) and the lowest (with Ratio Gain) accuracy on the same time, we believe that fold ten data's characteristics was well fitted for the information gain split but it might targeted the sensitivity of the ratio gain tree.

if we were looking at it in terms of accuracy we highly recommend using information gain with 10 folds (or any fold) to classifiy this model since it gave the highest accuracy out of the three algorithms.

## Clustering

Clustering involves organizing a set of objects so that those within the same group (cluster) share greater similarity with one another, compared to objects in different groups. This falls under the domain of unsupervised learning, as it lacks class labels. This section focuses on partitioning our dataset through the utilization of the k-means algorithm, exploring three distinct k-means values (2, 3, and 4). Throughout each iteration, we will assess the average silhouette, total within-cluster sum of square, and BCubed (precision and recall).

In terms of packages and methods, the K-means algorithm is employed to form the clusters, while fviz_cluster is utilized for plotting. For evaluation purposes, fviz_nbclust and fviz_silhouette are applied to calculate the silhouette values. Additionally, a custom method has been devised to compute BCubed precision and recall.

#Packages

```{r setup-libraries}
options(repos = c(CRAN = "https://cran.rstudio.com"))

# Install and load necessary libraries
install.packages("tidyverse")
install.packages("cluster")
install.packages("fastDummies")

library(tidyverse)
library(cluster)  # For clustering
library(fastDummies)  # For one-hot encoding

```

```{r}
# Load the data
data <- read.csv("credit_customers.csv")
true_labels <- data$class

# Identify categorical and numerical columns
categorical_cols <- names(data)[sapply(data, is.character)]
numerical_data <- data %>% select(-all_of(categorical_cols))

# One-hot encoding
encoded_data <- data %>% select(all_of(categorical_cols)) %>% 
  mutate(across(everything(), as.factor)) %>%
  model.matrix(~. - 1, data = .)

# Combine numerical and encoded data
data <- as.data.frame(cbind(numerical_data, encoded_data))
```

#Removing the class label(target) before we partition our data We have to remove the class label(target) since the clustering is an unsupervised learning.

```{r}

# Remove the class labels (column names)
colnames(data) <- NULL

```

Principal Component Analysis (PCA)

```{r}
# Handle missing values (if any)
data <- na.omit(data) 

# Z-score Standardization
data<- as.data.frame(scale(data))


 # Perform PCA
pca_result <- prcomp(data, center = FALSE, scale. = FALSE)  # Data is already standardized

# View summary of PCA results
print(summary(pca_result))

# Decide how many principal components to keep based on variance explained

cum_var_explained <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))
num_components <- which(cum_var_explained >= 0.8)[1]

# Select the number of principal components to use
data_pca <- pca_result$x[, 1:num_components]

loadings <- pca_result$rotation[, 1]
print(loadings)

data_pca <- pca_result$x[, 1]
k2_pca <- kmeans(data_pca, centers = 2, nstart = 25)

data_for_plot <- cbind(data_pca, rep(0, length(data_pca)))
data_pca_df <- as.data.frame(data_pca)
# Extracting the first three principal components
data_pca <- pca_result$x[, 1:3]

# Convert the PCA results to a data frame 
data_pca_df <- as.data.frame(data_pca)
```

Findings from PCA Output:

Principal Components:

Dataset has 50 principal components (PC1 to PC50). Variance Explained:

PC1 has the highest variability (1.79), followed by PC2, PC3, etc. The first 39 components capture about 99.7% of total variance. Loadings on PC1:

PC1 is influenced most by V2, V5, V1, and V3. Positive/negative signs indicate direction of influence. Diminishing Variance:

Standard deviations decrease for later components. PC50 has negligible variance (7.96e-16). Proportion of Variance:

Proportion of Variance decreases for later components. Cumulative Proportion:

Cumulative Proportion reaches 99.87% by PC47. Overall Implication: Selecting fewer components retains most information while reducing dimensionality.

#optimal number Elbow Method for determining the optimal number of clusters (K) in a k-means clustering algorithm. The Elbow Method helps identify a suitable value for K by plotting the Total Within Sum of Squares (WSS) against different values of K.

```{r}
#Elbow method
wss <- numeric(10) 

for (i in 1:10) {
  set.seed(123) # For reproducibility
  kmeans_result <- kmeans(data_pca_df, centers = i, nstart = 25)
  wss[i] <- kmeans_result$tot.withinss
}

# Creating the Elbow Plot
k_values <- 1:10 # Adjust the range as needed
elbow_plot <- data.frame(k_values, wss)
ggplot(elbow_plot, aes(x = k_values, y = wss)) + 
  geom_line() + 
  geom_point() + 
  theme_minimal() +
  labs(title = "Elbow Method", x = "Number of Clusters (K)", y = "Total Within Sum of Squares (WSS)")
optimal_k <- which.min(wss)
k_values <- c(optimal_k, optimal_k - 1, optimal_k + 1)
# Print the K values
cat("Optimal K value is:", optimal_k, "\n")
cat("K value immediately before the optimal K is:", optimal_k - 1, "\n")
cat("K value immediately after the optimal K is:", optimal_k + 1, "\n")
```

The output of this code will include the Elbow Plot and information about the optimal K value, as well as the K values immediately before and after the optimal K. The user can visually inspect the plot to identify the "elbow" point, which represents the optimal number of clusters for the k-means algorithm based on the WSS criterion

#calculate k-mean k=2,3,4 and plot

```{r}
# Load necessary library for visualization
library(factoextra)

# Clustering and plotting for k = 2, 3, and 4 as they were most significant from the elbow plot
set.seed(123)  # For reproducibility

dist_matrix <- dist(data)
# Clustering for k = 2
k2 <- kmeans(data_pca_df, centers = 2, nstart = 25)
fviz_cluster(k2, data = data_pca_df, geom = "point", 
             stand = FALSE, ellipse  = TRUE, 
             main = "Cluster Visualization (k=2)")
print(paste("Total WSS for k=2:", k2$tot.withinss))
sil_widths2 <- silhouette(k2$cluster, dist_matrix)
avg_sil_width2 <- mean(sil_widths2[, "sil_width"])
print(paste("Average Silhouette Width for k=2:", avg_sil_width2))

# Clustering for k = 3
k3 <- kmeans(data_pca_df, centers = 3, nstart = 25)
fviz_cluster(k3, data = data_pca_df, geom = "point", 
             stand = FALSE, ellipse  = TRUE, 
             main = "Cluster Visualization (k=3)")
print(paste("Total WSS for k=3:", k3$tot.withinss))
sil_widths3 <- silhouette(k3$cluster, dist_matrix)
avg_sil_width3 <- mean(sil_widths3[, "sil_width"])
print(paste("Average Silhouette Width for k=3:", avg_sil_width3))

# Clustering for k = 4
k4 <- kmeans(data_pca_df, centers = 4, nstart = 25)
fviz_cluster(k4, data = data_pca_df, geom = "point", 
             stand = FALSE, ellipse  = TRUE, 
             main = "Cluster Visualization (k=4)")
print(paste("Total WSS for k=4:", k4$tot.withinss))
sil_widths4 <- silhouette(k4$cluster, dist_matrix)
avg_sil_width4 <- mean(sil_widths4[, "sil_width"])
print(paste("Average Silhouette Width for k=4:", avg_sil_width4))



```

For k=2 :

• Total WSS (Within-Cluster Sum of Squares): 6122.20 • Average Silhouette Width: 0.0448

The Average Silhouette Score is the highest, indicating better-defined clusters. The Total WSS is also high, suggesting less compact clusters. BCubed Precision is moderate, and Recall is the highest among the three, suggesting a better overall cluster purity and completeness. The results for k=2 indicate that the clusters are reasonably well-separated, as reflected by a moderate average silhouette width. The total within-cluster sum of squares suggests substantial variation within the clusters.

For k=3 :

• Total WSS (Within-Cluster Sum of Squares): 4632.49 • Average Silhouette Width: 0.0382

When using k=3, the average silhouette width decreases, indicating less clearly defined clusters. The Total WSS is lower, suggesting more compact clusters. BCubed Precision improves, but Recall drops significantly, suggesting an increase in false negatives.

For k=4 :

• Total WSS (Within-Cluster Sum of Squares): 3792.62 • Average Silhouette Width: 0.0264

With k=4, the Average Silhouette Score is the lowest, indicating the poorest cluster definition. The Total WSS further decreases, again indicating more compact clusters. Both BCubed Precision and Recall are lower than for k=2, indicating lower overall clustering effectiveness.

```{r}
if (!require(fpc)) {
  install.packages("fpc")
}

library(cluster)
library(fpc)

# Load the data
data <- read.csv("credit_customers.csv")
# Specify the numeric and categorical attributes
numeric_attributes <- c("duration", "credit_amount", "age")
categorical_attributes <- setdiff(names(data), c("duration", "credit_amount", "age", "class"))

# Extract the relevant columns
numeric_data <- data[, numeric_attributes]
categorical_data <- data[, categorical_attributes]

# Check and convert columns in categorical_data to factors if needed
for (col in names(categorical_data)) {
  if (!is.factor(categorical_data[[col]]) && !is.numeric(categorical_data[[col]])) {
    categorical_data[[col]] <- as.factor(categorical_data[[col]])
  }
}

# Normalize numeric data 
normalized_numeric_data <- scale(numeric_data)

# Elbow method for determining the optimal number of clusters (k-means)
wss <- numeric(length = 10)
for (k in 1:10) {
  kmeans_model <- kmeans(normalized_numeric_data, centers = k, nstart = 10)
  wss[k] <- sum(kmeans_model$withinss)
}

# Plot the Elbow method graph with adjusted margins
par(mar = c(5, 4, 4, 2) + 0.1)
plot(1:10, wss, type = "b", main = "Elbow Method for Optimal k (k-means)",
     xlab = "Number of Clusters (k)", ylab = "Within-Cluster Sum of Squares (WSS)")

# Choose three different sizes of K based on the Elbow plot
optimal_k <- which.min(wss)
k_values <- c(optimal_k, optimal_k - 1, optimal_k + 1)

# Perform k-medoids clustering on categorical data
dissimilarity_matrix <- daisy(categorical_data, metric = "gower")
kmedoids_model <- pam(dissimilarity_matrix, k = 3)

# Print the k-medoids clustering results
cat("\nK-Medoids Clustering Results:\n")
print(kmedoids_model)

# Evaluate clustering using Silhouette coefficient for k-medoids
silhouette_kmedoids <- silhouette(kmedoids_model$cluster, dissimilarity_matrix)
cat("\nSilhouette Coefficient for K-Medoids Clustering:\n")
print(silhouette_kmedoids)

# Evaluate BCubed precision and recall for k-medoids
bcubed_kmedoids <- cluster.stats(dissimilarity_matrix, kmedoids_model$cluster)$bcubed
cat("\nBCubed Precision and Recall for K-Medoids Clustering:\n")
print(bcubed_kmedoids)


```

Bcuped recall and percision

```{r}
cluster_assignments <- c(k2$cluster)
ground_truth_labels <- c(true_labels)

data <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)

# Function to calculate BCubed precision and recall
calculate_bcubed_metrics <- function(data) {
  n <- nrow(data)
  precision_sum <- 0
  recall_sum <- 0

  for (i in 1:n) {
    cluster <- data$cluster[i]
    label <- data$label[i]
    
# Count the number of items from the same category within the same cluster
same_category_same_cluster <- sum(data$label[data$cluster == cluster] == label)
    
# Count the total number of items in the same cluster
total_same_cluster <- sum(data$cluster == cluster)
    
# Count the total number of items with the same category
total_same_category <- sum(data$label == label)
    
# Calculate precision and recall for the current item and add them to the sums
precision_sum <- precision_sum + same_category_same_cluster /total_same_cluster
recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }

  # Calculate average precision and recall
  precision <- precision_sum / n
  recall <- recall_sum / n

  return(list(precision = precision, recall = recall))
}

# Calculate BCubed precision and recall
metrics <- calculate_bcubed_metrics(data)

# Extract precision and recall from the metrics
precision <- metrics$precision
recall <- metrics$recall

# Print the results
cat("BCubed Precision:", precision, "\n")
cat("BCubed Recall:", recall, "\n")


```

```{r}
cluster_assignments <- c(k3$cluster)
ground_truth_labels <- c(true_labels)

data <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)

# Function to calculate BCubed precision and recall
calculate_bcubed_metrics <- function(data) {
  n <- nrow(data)
  precision_sum <- 0
  recall_sum <- 0

  for (i in 1:n) {
    cluster <- data$cluster[i]
    label <- data$label[i]
    
# Count the number of items from the same category within the same cluster
same_category_same_cluster <- sum(data$label[data$cluster == cluster] == label)
    
# Count the total number of items in the same cluster
total_same_cluster <- sum(data$cluster == cluster)
    
# Count the total number of items with the same category
total_same_category <- sum(data$label == label)
    
# Calculate precision and recall for the current item and add them to the sums
precision_sum <- precision_sum + same_category_same_cluster /total_same_cluster
recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }

  # Calculate average precision and recall
  precision <- precision_sum / n
  recall <- recall_sum / n

  return(list(precision = precision, recall = recall))
}

# Calculate BCubed precision and recall
metrics <- calculate_bcubed_metrics(data)

# Extract precision and recall from the metrics
precision3 <- metrics$precision
recall3 <- metrics$recall

# Print the results
cat("BCubed Precision:", precision3, "\n")
cat("BCubed Recall:", recall3, "\n")


```

```{r}
cluster_assignments <- c(k4$cluster)
ground_truth_labels <- c(true_labels)

data <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)

# Function to calculate BCubed precision and recall
calculate_bcubed_metrics <- function(data) {
  n <- nrow(data)
  precision_sum <- 0
  recall_sum <- 0

  for (i in 1:n) {
    cluster <- data$cluster[i]
    label <- data$label[i]
    
# Count the number of items from the same category within the same cluster
same_category_same_cluster <- sum(data$label[data$cluster == cluster] == label)
    
# Count the total number of items in the same cluster
total_same_cluster <- sum(data$cluster == cluster)
    
# Count the total number of items with the same category
total_same_category <- sum(data$label == label)
    
# Calculate precision and recall for the current item and add them to the sums
precision_sum <- precision_sum + same_category_same_cluster /total_same_cluster
recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }

  # Calculate average precision and recall
  precision <- precision_sum / n
  recall <- recall_sum / n

  return(list(precision = precision, recall = recall))
}

# Calculate BCubed precision and recall
metrics <- calculate_bcubed_metrics(data)

# Extract precision and recall from the metrics
precision4 <- metrics$precision
recall4 <- metrics$recall

# Print the results
cat("BCubed Precision:", precision, "\n")
cat("BCubed Recall:", recall, "\n")


results <- data.frame(
  K = c(2, 3, 4),
  Average_Silhouette = c(avg_sil_width2, avg_sil_width3, avg_sil_width4),
  Total_WSS = c(k2$tot.withinss, k3$tot.withinss, k4$tot.withinss),
  BCubed_Precision = c(precision, precision3, precision4),
  BCubed_Recall = c(recall,recall3, recall4)
)

# Print the results table
print(results)
```

## Evaluation and Comparison

## Evaluation for classification

|             |       | 10       |            |       | 5        |            |       | 3        |            |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|
|             | IG    | IG ratio | gini index | IG    | IG ratio | gini index | IG    | IG ratio | gini index |
| Accuracy    | 71.72 | 68.69    | 73.74%     | 68.34 | 69.35    | 71.36      | 70    | 71.60    | 71.60      |
| precision   | 86.96 | 56.80    | 59.46%     | 79.14 | 52.27    | 59.04      | 87.88 | 86.58    | 56.95      |
| sensitivity | 75.95 | 40.13    | 40.82%     | 76.39 | 38.49    | 39.80      | 74.09 | 17.67    | 42.13      |
| specificity | 55    | 86.19    | 88.06%     | 47.27 | 85.18    | 87.34      | 50    | 93.95    | 85.90      |

best among all:

1.  **Scenario 10 IG:**

    -   Accuracy: 71.72%

    -   Precision: 86.96%

    -   Sensitivity: 75.95%

    -   Specificity: 55.00%

    -   This scenario has the highest accuracy and precision, making it a strong performer in correctly classifying instances, especially positive ones. However, the specificity is comparatively lower.

2.  **Scenario 5 IG:**

    -   Accuracy: 68.34%

    -   Precision: 79.14%

    -   Sensitivity: 76.39%

    -   Specificity: 47.27%

    -   This scenario has a good balance between precision and sensitivity, but the overall accuracy is slightly lower. The specificity is also relatively low.

3.  **Scenario 3 IG:**

    -   Accuracy: 70.00%

    -   Precision: 87.88%

    -   Sensitivity: 74.09%

    -   Specificity: 50.00%

    -   This scenario excels in precision and has a moderate accuracy. Sensitivity is also reasonable, but specificity is relatively low.

to decide what is the the best among all we will used the balanced approach (f-1 score) which is the harmonic mean of precision and recall, which means that the F1 score will tell us the model's balanced ability to both capture positive cases (recall) and be accurate with the cases it does capture (precision).

we went with this approach because in credit risk assessment, the consequences of misclassifying instances in either class can be significant. A balanced approach ensures that both positive (good credit) and negative (bad credit) cases receive equal consideration.

F1=2× Precision×Recall/Precision+Recall

```{r}
# F1 score results
precision <- c(86.96, 79.14, 87.88)
sensitivity <- c(75.95, 76.39, 74.09)
f1_score <- 2 * (precision * sensitivity) / (precision + sensitivity)

# Create a data frame
results <- data.frame(
  Scenario = c("10 folds IG", "5 folds IG", "3 folds IG"),
  Precision = precision,
  Sensitivity = sensitivity,
  F1_Score = f1_score
)

# Print the results
print(results)
```

Based on the F1 scores, 10 Folds information gain has the highest F1 score (81.08295), making it the best-performing scenario

## Evaluation for clustering:

|     |                                    | K=2 (BEST) | K=3    | K=4    |
|-----|------------------------------------|------------|--------|--------|
|     | Average Silhouette width           | 0.0448     | 0.0382 | 0.0264 |
|     | total within-cluster sum of square | 6122       | 4632   | 3793   |
|     | BCubed precision                   | 0.583      | 0.640  | 0.626  |
|     | BCubed recall                      | 0.504      | 0.392  | 0.290  |

For\
k=2\
K=2, the Average Silhouette Score is the highest, indicating better-defined clusters. The Total WSS is also high, suggesting less compact clusters. BCubed Precision is moderate, and Recall is the highest among the three, suggesting a better overall cluster purity and completeness.\

\
For\
k=3\
K=3, the Average Silhouette Score decreases slightly, indicating less clearly defined clusters. The Total WSS is lower, suggesting more compact clusters. BCubed Precision improves, but Recall drops significantly, suggesting an increase in false negatives.\

\
For\
k=4\
K=4, the Average Silhouette Score is the lowest, indicating the poorest cluster definition. The Total WSS further decreases, again indicating more compact clusters. Both BCubed Precision and Recall are lower than for\
k=2\
K=2, indicating lower overall clustering effectiveness.\

\
Based on these metrics,\
k=2\
K=2 seems to be the best choice as it has the highest Average Silhouette Score, indicating well-defined clusters, and the highest BCubed Recall, suggesting better completeness of clusters with respect to the true classes. While its Total WSS is higher than for\
k=3\
K=3 or k=4\
K=4, this is not necessarily a drawback, as overly compact clusters can indicate overfitting. The trade-off between these metrics depends on the specific needs and context of your clustering application.

In summary, the results consistently highlight that k=2 is the preferable choice for clustering. It not only demonstrates the highest Average Silhouette Score, indicating well-defined clusters, but also boasts the highest BCubed Recall, suggesting better completeness of clusters concerning the true classes. Although the Total WSS is higher for k=2 compared to k=3 and k=4, this is not necessarily a drawback, as overly compact clusters may indicate overfitting. The trade-off between these metrics depends on the specific needs and context of the clustering application.

## Findings

Based on the nature of our problem, our model needs to come up with one result which is wither the customer we're assisting has good credit or bad credit. We already have a distinct and well defined classes and want to predict it for unseen/new customers. We believe that classification methods are more suited for our dataset. The IG ID3 with 10 folds is what we choose to go with. Another factor is since our class label is known "the class attribute" which is better suited in the classification process, we didn't have to remove it like we did in the clustering process. for that reason we believe that the classification is better for our data set giving us the best results in comparison with clustering.

the best performing clustering.

the best performing classification algorithm.
